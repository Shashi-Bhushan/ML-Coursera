In Linear regression, we have training set.
m is the number of training examples.

Hypothesis that we use to make predictions is a linear function.

h subscript theta (x) = θ0 and θ1(x)

θ0 and θ1 are Parameters of the model.

=========================

HOW TO CHOOSE THESE TWO PARAMETERS VALUES

I want to choose my hypothesis function h such that my parameters (θ0 and θ1) are minimum.
Also, h(x(i)) - y should be minimum i.e. hypothesis of x(i) - Estimated price y should be minimum.

So, in order to assess the accuracy of our chosen hypothesis, we use a Cost Function.

=========================

COST FUNCTION

We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.

J(θ0, θ1) = (1/2m) ∑i=1 to m [y^i−yi]square 
		  = (1/2m) ∑i=1 to m [hθ(xi)−yi]square


To break it apart, it is (1/2 x¯) where (x¯) is the mean of the squares of [hθ(xi)−yi] , 
or the difference between the predicted value and the actual value.
The mean is halved (1/2) as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the 1/2 term.

This Cost Function is also called Squared Error Function or Mean Error Function.
Squared Error cost function is the most commonly used Error function for Linear Regression Problems.

=========================

WHY COST FUNCTION

Because when we are given m data points, we need to device a hypothesis formulae, which can correctly interpolate any values of y for a given x.
Now, these data points are not some standard data points, which follows a specific formulae.

For Eg, say we have formulae y = 2x. Different values of x and y for this example can be
	x = 1, y = 2
	x = 2, y = 4
	x = 3, y = 6
	and so on.
Here, there is no margin of error because each y is derived from a formulae, which takes x as input.
But, when we have to device a formulae for the data points already given, there is a margin of error. This margin of error is measured by our Cost Function J(θ0, θ1) and We seek to minimize this error margin so our interpolated values are as close to reality as possible.