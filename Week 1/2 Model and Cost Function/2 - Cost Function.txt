In Linear regression, we have training set.
m is the number of training examples.

Hypothesis that we use to make predictions is a linear function.

h subscript theta (x) = theta subscript 0 + theta subscript 1 (x)

(Theta zero) and (Theta one) are Parameters of the model.

=========================

HOW TO CHOOSE THESE TWO PARAMETERS VALUES

I want to choose my hypothesis function h such that my parameters (θ0 and θ1) are minimum.
Also, h(x(i)) - y should be minimum i.e. hypothesis of x(i) - Estimated price y should be minimum.

So, in order to assess the accuracy of our chosen hypothesis, we use a Cost Function.

=========================

COST FUNCTION

We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.

J(θ0, θ1) = (1/2m) ∑i=1 to m [y^i−yi]square 
		  = (1/2m) ∑i=1 to m [hθ(xi)−yi]square


To break it apart, it is (1/2 x¯) where (x¯) is the mean of the squares of [hθ(xi)−yi] , 
or the difference between the predicted value and the actual value.
The mean is halved (1/2) as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the 1/2 term.

This Cost Function is also called Squared Error Function or Mean Error Function.
Squared Error cost function is the most commonly used Error function for Linear Regression Problems.