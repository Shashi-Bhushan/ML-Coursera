Hypothesis Representation
--------------------
We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for h(sub θ)x to take values larger than 1 or smaller than 0 when we know that y ∈ {0, 1}. 

To fix this, let’s change the form for our hypotheses h(sub θ) to satisfy 0 ≤ h(sub θ) ≤1. 
This is accomplished by plugging θᵀx into the Logistic Function.

Our new form uses the "Sigmoid Function," also called the "Logistic Function":

h(sub θ)x = g(θᵀx)
where    z = θᵀx
      g(z) = 1 / (1+e⁻ᶻ)

The function g(z), shown here, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.

h(sub θ)x will give us the probability that our output is 1. For example, h(sub θ)x = 0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).

h(sub θ)x = P(y = 1|x;θ) = 1 − P(y = 0|x;θ)
Also, P(y = 0|x;θ) + P(y = 1|x;θ) = 1